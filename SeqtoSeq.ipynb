{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqtoSeq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girish445ai/Recurrent_Neural_networks/blob/main/SeqtoSeq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAtERU36sYPZ",
        "outputId": "9cb84ea0-d443-4df2-ab2d-566d3f0cfeb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 462 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 22.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 27.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 917 kB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import shuffle\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, SimpleRNN, Dropout, Activation, dot, concatenate, TimeDistributed\n",
        "from keras.models import Model\n",
        "\n",
        "!pip3 install tensorflow -qqq\n",
        "!pip3 install wandb -qqq\n",
        "import wandb\n",
        "!wandb login\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL6Ptq11sdUx",
        "outputId": "c5e2b22f-9804-4471-8633-8f998958ae0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-17 04:51:44--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.97.128, 108.177.125.128, 142.250.157.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.97.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   129MB/s    in 15s     \n",
            "\n",
            "2022-04-17 04:52:00 (124 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | tar xopf dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "id": "24e3Fnn_3IOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The folder containing the datasets to be used in this program\n",
        "!ls dakshina_dataset_v1.0/te/lexicons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lzPR2nO3tIn",
        "outputId": "94d3d5d8-7170-466a-a22e-8482b4805344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "te.translit.sampled.dev.tsv   te.translit.sampled.train.tsv\n",
            "te.translit.sampled.test.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = './dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.test.tsv'\n",
        "val_path = './dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv'\n",
        "train_path = './dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv'"
      ],
      "metadata": {
        "id": "KtWZEddis-p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "START_CHAR = '\\t'\n",
        "END_CHAR = '\\n'\n",
        "BLANK_CHAR = ' '"
      ],
      "metadata": {
        "id": "1mMNDdHauclw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input and output pairs "
      ],
      "metadata": {
        "id": "46yo4e1eOdMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reading_data(data_path, characters = False):\n",
        "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [line.split(\"\\t\") for line in f.read().split(\"\\n\") if line != '']\n",
        "    \n",
        "    input, target = [val[1] for val in lines], [val[0] for val in lines] # READING input/output samples as string when characters= False \n",
        "    if characters:\n",
        "        input, target = [list(inp_str) for inp_str in input], [list(tar_str) for tar_str in target]\n",
        "    return input, target # READING input/output samples as list of characters when characters= True \n"
      ],
      "metadata": {
        "id": "EOq0weJm4ZIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the data"
      ],
      "metadata": {
        "id": "7_DOWy6Zdz9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processing_data(input, enc_timesteps, input_char_enc, target = None, dec_timesteps = None, target_char_enc = None):\n",
        "    # Returns the input and target data word embeddings. \n",
        "    \n",
        "    # BLANK_CHAR is the space input (if spaces are in input) \n",
        "    encoder_input = np.array([[input_char_enc[ch] for ch in string] + [input_char_enc[BLANK_CHAR]] * (enc_timesteps - len(string)) for string in input])\n",
        "\n",
        "    decoder_input, decoder_target = None, None\n",
        "    if target is not None and dec_timesteps is not None and target_char_enc is not None:\n",
        "        # START_CHAR is the start of sequence, END_CHAR is end of sequence\n",
        "        decoder_input = np.array([[target_char_enc[START_CHAR]] + [target_char_enc[ch] for ch in string] + [target_char_enc[END_CHAR]] \n",
        "                                    + [target_char_enc[BLANK_CHAR]] * (dec_timesteps - len(string) - 2) for string in target])\n",
        "        decoder_target = np.zeros((decoder_input.shape[0], dec_timesteps, len(target_char_enc)), dtype='float32')\n",
        "\n",
        "        for i in range(decoder_input.shape[0]):\n",
        "            for t, char_ind in enumerate(decoder_input[i]):\n",
        "                if t > 0:\n",
        "                    decoder_target[i,t-1,char_ind] = 1.0\n",
        "            decoder_target[i,t:,target_char_enc[BLANK_CHAR]] = 1.0\n",
        "\n",
        "    return encoder_input, decoder_input, decoder_target\n",
        "\n",
        "\n",
        "def encode_decode_characters(train_input, train_target, val_input, val_target):\n",
        "    # Returns the encoding for characters to integer (as a dictionary) and decoding for integers to characters (as a list) for input and target data\n",
        "\n",
        "    # Encoding and decoding of input vocabulary\n",
        "    input_char_enc = {}\n",
        "    input_char_dec = []\n",
        "    max_encoder_seq_length = 1\n",
        "    for string in train_input + val_input:\n",
        "        max_encoder_seq_length = max(max_encoder_seq_length, len(string))\n",
        "        for char in string:\n",
        "            if char not in input_char_enc:\n",
        "                input_char_enc[char] = len(input_char_dec)\n",
        "                input_char_dec.append(char)\n",
        "    if BLANK_CHAR not in input_char_enc:\n",
        "        input_char_enc[BLANK_CHAR] = len(input_char_dec)\n",
        "        input_char_dec.append(BLANK_CHAR)\n",
        "\n",
        "    # Encoding and decoding of target vocabulary\n",
        "    target_char_enc = {}\n",
        "    target_char_dec = []\n",
        "    target_char_enc[START_CHAR] = len(target_char_dec)\n",
        "    target_char_dec.append(START_CHAR)\n",
        "    max_decoder_seq_length = 1\n",
        "    for string in train_target + val_target:\n",
        "        max_decoder_seq_length = max(max_decoder_seq_length, len(string)+2)\n",
        "        for char in string:\n",
        "            if char not in target_char_enc:\n",
        "                target_char_enc[char] = len(target_char_dec)\n",
        "                target_char_dec.append(char)\n",
        "    target_char_enc[END_CHAR] = len(target_char_dec)\n",
        "    target_char_dec.append(END_CHAR)\n",
        "    if ' ' not in target_char_enc:\n",
        "        target_char_enc[BLANK_CHAR] = len(target_char_dec)\n",
        "        target_char_dec.append(BLANK_CHAR)\n",
        "\n",
        "    print(\"Number of training samples:\", len(train_input))\n",
        "    print(\"Number of validation samples:\", len(val_input))\n",
        "    print(\"Number of unique input tokens:\", len(input_char_dec))\n",
        "    print(\"Number of unique output tokens:\", len(target_char_dec))\n",
        "    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "    return input_char_enc, input_char_dec, target_char_enc, target_char_dec, max_encoder_seq_length, max_decoder_seq_length\n"
      ],
      "metadata": {
        "id": "0JzWJBuBd1OG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}