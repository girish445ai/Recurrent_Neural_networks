{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girish445ai/Recurrent_Neural_networks/blob/main/TESTING_Transliteration_without_attention_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTK4IFUhLr8N"
      },
      "source": [
        "### Importing Libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "apeRGrBs6yH8"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow \n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Dropout, SimpleRNN\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "from math import log1p "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "0NsYPEEpM5cU"
      },
      "outputs": [],
      "source": [
        "#%pip install wandb -q\n",
        "#import wandb\n",
        "#from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6lJCkKMLwDe"
      },
      "source": [
        "### Unzipping the dataset\n",
        "\n",
        "Lexicons for Latin-Telugu are taken from Google's Dakshina dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhpFjBqec5sk",
        "outputId": "3ad00d5a-3a32-4c13-e5e5-5ca39f47a975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-08 15:56:28--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.76.128, 142.251.5.128, 2a00:1450:400c:c00::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.76.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar.2’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   225MB/s    in 9.4s    \n",
            "\n",
            "2022-05-08 15:56:38 (204 MB/s) - ‘dakshina_dataset_v1.0.tar.2’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downloading dakshina dataset\n",
        "!yes | wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "A8V3WTrkfF66"
      },
      "outputs": [],
      "source": [
        "# Unzipping dataset\n",
        "!yes | tar xopf dakshina_dataset_v1.0.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9IlHm7CfO_I",
        "outputId": "25db2a75-fd3d-4f13-dc2f-064f556d1c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "te.translit.sampled.dev.tsv   te.translit.sampled.train.tsv\n",
            "te.translit.sampled.test.tsv\n"
          ]
        }
      ],
      "source": [
        "# The dakshina dataset has the lexicons that are used in this program.\n",
        "!ls dakshina_dataset_v1.0/te/lexicons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "6x95CxiMFl54"
      },
      "outputs": [],
      "source": [
        "print_data = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPoGXywuL1kD"
      },
      "source": [
        "## Reading the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBzvL2e20Sx-",
        "outputId": "56c4e0d0-0da9-45da-a5cf-0f7b48a39027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  58550\n",
            "Number of validation samples:  5683\n",
            "Number of testing samples:  5683\n"
          ]
        }
      ],
      "source": [
        "train_path = \"./dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\"\n",
        "dev_path = \"./dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\"\n",
        "test_path = \"./dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\"\n",
        "\n",
        "def reading_data(corpus_file):\n",
        "  # function reads the raw text of words and returns native versions of words\n",
        "  telugu_words = []\n",
        "  latin_words = []\n",
        "  with io.open(corpus_file, encoding ='utf-8') as f:\n",
        "    for line in f:\n",
        "      if '\\t' not in line:\n",
        "        continue\n",
        "      tokens = line.rstrip().split(\"\\t\")\n",
        "      latin_words.append(tokens[1])\n",
        "      telugu_words.append(tokens[0])\n",
        "  return latin_words, telugu_words\n",
        "\n",
        "train_source, train_target = reading_data(train_path)\n",
        "val_source, val_target = reading_data(dev_path)\n",
        "test_source, test_target = reading_data(test_path)\n",
        "\n",
        "print(\"Number of training samples: \", len(train_source))\n",
        "print(\"Number of validation samples: \", len(val_source))\n",
        "print(\"Number of testing samples: \", len(test_source))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mC2ICFKRUZA",
        "outputId": "2bcd30e1-70dc-493d-8fc1-04e5db6b474e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of samples: 58550\n",
            "No of unique input tokens: 27\n",
            "No of unique output tokens: 66\n",
            "Maximum sequence length for inputs: 25\n",
            "Maximum sequence length for outputs: 22\n",
            "Maximum sequence length for val inputs: 21\n",
            "Maximum sequence length for val outputs: 21\n",
            "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "[' ', 'B', 'E', 'ం', 'ః', 'అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ', 'క', 'ఖ', 'గ', 'ఘ', 'చ', 'ఛ', 'జ', 'ఝ', 'ఞ', 'ట', 'ఠ', 'డ', 'ఢ', 'ణ', 'త', 'థ', 'ద', 'ధ', 'న', 'ప', 'ఫ', 'బ', 'భ', 'మ', 'య', 'ర', 'ఱ', 'ల', 'ళ', 'వ', 'శ', 'ష', 'స', 'హ', 'ా', 'ి', 'ీ', 'ు', 'ూ', 'ృ', 'ె', 'ే', 'ై', 'ొ', 'ో', 'ౌ', '్', '\\u200c']\n"
          ]
        }
      ],
      "source": [
        "arr = np.arange(len(train_source))\n",
        "np.random.shuffle(arr)\n",
        "arr1 = np.arange(len(val_source))\n",
        "np.random.shuffle(arr1)\n",
        "\n",
        "input_chars = set()\n",
        "target_chars = set()\n",
        "input_lexicons_nextstep = []\n",
        "target_lexicons_nextstep = []\n",
        "val_input_lexicons_nextstep = []\n",
        "val_target_lexicons_nextstep = []\n",
        "\n",
        "for (input_text, target_text) in zip(train_source, train_target):\n",
        "    # \"tab\" is the \"start sequence\" characte ,\"\\n\" is \"end sequence\" character.\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    input_lexicons_nextstep.append(input_text)\n",
        "    target_lexicons_nextstep.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_chars:\n",
        "            input_chars.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_chars:\n",
        "            target_chars.add(char)\n",
        "\n",
        "for (input_text, target_text) in zip(val_source, val_target):\n",
        "    # \"tab\" is the \"start sequence\" character ,\"\\n\" is \"end sequence\" character.\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    val_input_lexicons_nextstep.append(input_text)\n",
        "    val_target_lexicons_nextstep.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_chars:\n",
        "            input_chars.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_chars:\n",
        "            target_chars.add(char)\n",
        "\n",
        "input_lexicons = []\n",
        "target_lexicons = []\n",
        "\n",
        "for i in range(len(train_source)):\n",
        "    input_lexicons.append(input_lexicons_nextstep[arr[i]])\n",
        "    target_lexicons.append(target_lexicons_nextstep[arr[i]])\n",
        "\n",
        "val_input_lexicons = []\n",
        "val_target_lexicons = []\n",
        "\n",
        "for i in range(len(val_source)):\n",
        "    val_input_lexicons.append(val_input_lexicons_nextstep[arr1[i]])\n",
        "    val_target_lexicons.append(val_target_lexicons_nextstep[arr1[i]])\n",
        "\n",
        "input_chars.add(\" \")\n",
        "target_chars.add(\" \")\n",
        "\n",
        "input_chars = sorted(list(input_chars))\n",
        "target_chars = sorted(list(target_chars))\n",
        "\n",
        "\n",
        "no_enc_tokens = len(input_chars)\n",
        "no_dec_tokens = len(target_chars)\n",
        "enc_seq_length = max([len(txt) for txt in input_lexicons])\n",
        "dec_seq_length = max([len(txt) for txt in target_lexicons])\n",
        "val_max_encoder_seq_length = max([len(txt) for txt in val_input_lexicons])\n",
        "val_max_decoder_seq_length = max([len(txt) for txt in val_target_lexicons])\n",
        "\n",
        "\n",
        "\n",
        "print(\"No of samples:\", len(input_lexicons))\n",
        "print(\"No of unique input tokens:\", no_enc_tokens)\n",
        "print(\"No of unique output tokens:\", no_dec_tokens)\n",
        "print(\"Maximum sequence length for inputs:\", enc_seq_length)\n",
        "print(\"Maximum sequence length for outputs:\", dec_seq_length)\n",
        "print(\"Maximum sequence length for val inputs:\", val_max_encoder_seq_length)\n",
        "print(\"Maximum sequence length for val outputs:\", val_max_decoder_seq_length)\n",
        "\n",
        "print(input_chars)\n",
        "print(target_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f7y6C51r1QI",
        "outputId": "a3c3a968-cdc2-407b-9acd-32787fc4c869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ghattamaneni', 'emuka', 'thare', 'angeekaristaaru', 'nirvahinchadaniki', 'uttheej', 'samsthalato']\n",
            "['Bఘట్టమనేనిE', 'BఎముకE', 'Bదేర్E', 'Bఅంగీకరిస్తారుE', 'Bనిర్వహించడానికిE', 'Bఉత్తేజ్E', 'Bసంస్థలతోE']\n"
          ]
        }
      ],
      "source": [
        "print(input_lexicons[123:130])\n",
        "print(target_lexicons[123:130])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwPNb93PRwHT"
      },
      "source": [
        "**Training** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtHaPQPw6VuP",
        "outputId": "1762f9f2-3ec1-4ba3-9787-e24cee974276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "{' ': 0, 'B': 1, 'E': 2, 'ం': 3, 'ః': 4, 'అ': 5, 'ఆ': 6, 'ఇ': 7, 'ఈ': 8, 'ఉ': 9, 'ఊ': 10, 'ఋ': 11, 'ఎ': 12, 'ఏ': 13, 'ఐ': 14, 'ఒ': 15, 'ఓ': 16, 'ఔ': 17, 'క': 18, 'ఖ': 19, 'గ': 20, 'ఘ': 21, 'చ': 22, 'ఛ': 23, 'జ': 24, 'ఝ': 25, 'ఞ': 26, 'ట': 27, 'ఠ': 28, 'డ': 29, 'ఢ': 30, 'ణ': 31, 'త': 32, 'థ': 33, 'ద': 34, 'ధ': 35, 'న': 36, 'ప': 37, 'ఫ': 38, 'బ': 39, 'భ': 40, 'మ': 41, 'య': 42, 'ర': 43, 'ఱ': 44, 'ల': 45, 'ళ': 46, 'వ': 47, 'శ': 48, 'ష': 49, 'స': 50, 'హ': 51, 'ా': 52, 'ి': 53, 'ీ': 54, 'ు': 55, 'ూ': 56, 'ృ': 57, 'ె': 58, 'ే': 59, 'ై': 60, 'ొ': 61, 'ో': 62, 'ౌ': 63, '్': 64, '\\u200c': 65}\n"
          ]
        }
      ],
      "source": [
        "# input_token_index is a dictionary containg the latin characters.\n",
        "# target_token_index is a dictionary containing the characters of target language here telugu.\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_chars)])\n",
        "print(input_token_index)\n",
        "print(target_token_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "y3i7ykYbtQ2K"
      },
      "outputs": [],
      "source": [
        "# Encoder Input Sequences are padded to a maximum length of MAX encoder SeqLen characters. \n",
        "enc_input_data = np.zeros(\n",
        "    (len(input_lexicons), enc_seq_length), dtype=\"float32\"\n",
        ")\n",
        "dec_input_data = np.zeros(\n",
        "    (len(input_lexicons), dec_seq_length), dtype=\"float32\"\n",
        ")\n",
        "dec_target_data = np.zeros(\n",
        "    (len(input_lexicons), dec_seq_length, no_dec_tokens), dtype=\"float32\"\n",
        ")\n",
        "#Decoder Target Sequences are Padded to a maximum length of max_decoder SeqLen characters with a vocabulary of sizeofTeluguVocab different characters. \n",
        "for i, (input_text, target_text) in enumerate(zip(input_lexicons, target_lexicons)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        enc_input_data[i, t] = input_token_index[char]\n",
        "    enc_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        # dec_target_data is ahead of dec_input_data by one timestep\n",
        "        dec_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # dec_target_data will not include the start character.\n",
        "            dec_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    dec_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    dec_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "\n",
        "val_enc_input_data = np.zeros(\n",
        "    (len(input_lexicons), val_max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_dec_input_data = np.zeros(\n",
        "    (len(input_lexicons), val_max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_dec_target_data = np.zeros(\n",
        "    (len(input_lexicons), val_max_decoder_seq_length, no_dec_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(val_input_lexicons, val_target_lexicons)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        # Enumerate() method adds a counter to an iterable and returns it in a form of enumerating object. \n",
        "        # This enumerated object can then be used directly for loops or converted into a list of tuples using the list() method.\n",
        "        val_enc_input_data[i, t] = input_token_index[char]\n",
        "    val_enc_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        val_dec_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # dec_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            val_dec_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    val_dec_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    val_dec_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOUdwNGG39qb",
        "outputId": "f039a64b-e2a6-4248-dcde-3cf6013c6f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: ' ', 1: 'B', 2: 'E', 3: 'ం', 4: 'ః', 5: 'అ', 6: 'ఆ', 7: 'ఇ', 8: 'ఈ', 9: 'ఉ', 10: 'ఊ', 11: 'ఋ', 12: 'ఎ', 13: 'ఏ', 14: 'ఐ', 15: 'ఒ', 16: 'ఓ', 17: 'ఔ', 18: 'క', 19: 'ఖ', 20: 'గ', 21: 'ఘ', 22: 'చ', 23: 'ఛ', 24: 'జ', 25: 'ఝ', 26: 'ఞ', 27: 'ట', 28: 'ఠ', 29: 'డ', 30: 'ఢ', 31: 'ణ', 32: 'త', 33: 'థ', 34: 'ద', 35: 'ధ', 36: 'న', 37: 'ప', 38: 'ఫ', 39: 'బ', 40: 'భ', 41: 'మ', 42: 'య', 43: 'ర', 44: 'ఱ', 45: 'ల', 46: 'ళ', 47: 'వ', 48: 'శ', 49: 'ష', 50: 'స', 51: 'హ', 52: 'ా', 53: 'ి', 54: 'ీ', 55: 'ు', 56: 'ూ', 57: 'ృ', 58: 'ె', 59: 'ే', 60: 'ై', 61: 'ొ', 62: 'ో', 63: 'ౌ', 64: '్', 65: '\\u200c'}\n"
          ]
        }
      ],
      "source": [
        "# Feeding the characters in reverse order (bidirectional) for better processing\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "print(reverse_target_char_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjskNaet7bgj",
        "outputId": "cf443cb7-a325-409e-d728-9234b29a54cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 4. 18. 21.  7. 22.  9. 19.  8.  1. 25.  1. 13.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.]\n",
            "[ 1. 34. 57. 20. 64. 47. 53. 49. 42.  3.  2.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(enc_input_data[1])\n",
        "print(dec_input_data[1])\n",
        "print(dec_target_data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui9pFxRvSdeT"
      },
      "source": [
        "For Validation and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVGsHbTJn807",
        "outputId": "f75ce0eb-1450-4001-bfba-a49a68ea0f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1. 42. 55. 34. 64. 35. 41. 55. 36. 18. 55.  2.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(val_dec_input_data[26])\n",
        "print(val_dec_target_data[26])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "X7q6xNssrRwb"
      },
      "outputs": [],
      "source": [
        "x_test = val_enc_input_data\n",
        "y_test = val_target_lexicons"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL TRAINING "
      ],
      "metadata": {
        "id": "fsl1l9XK_jm0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "AvI1fI7UOPMT"
      },
      "outputs": [],
      "source": [
        "class GJRNN(object):\n",
        "  def __init__(self,cell_type = 'RNN',in_emb = 32, hidden_size=32, learning_rate= 1e-3, \n",
        "               dropout=0.4,pred_type ='greedy',epochs = 10, batch_size = 32,beam_width = 5,\n",
        "               num_enc = 1,num_dec = 1):\n",
        "    # RNN class with initialization of different parameters.\n",
        "    self.cell_type = cell_type\n",
        "    self.in_emb = in_emb\n",
        "    self.hidden_size = hidden_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.dropout = dropout\n",
        "    self.pred_type = pred_type\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.beam_width = beam_width\n",
        "    self.num_enc = num_enc\n",
        "    self.num_dec = num_dec\n",
        "\n",
        "  def model_build(self,enc_input_data,dec_input_data,dec_target_data,x_test, y_test):\n",
        "    enc_inputs = Input(shape=(None, ),name = 'Enc_inputs')\n",
        "\n",
        "    # The Embedding layer takes the input vocab of size of number encoder tokens, and\n",
        "    # returns the output embedding of size in_emb.(embedding dimension)\n",
        "    enc_emb =  Embedding(no_enc_tokens, self.in_emb , mask_zero = True,name = 'Enc_emb')(enc_inputs)\n",
        "\n",
        "    enc_outputs = enc_emb\n",
        "    if self.cell_type == 'LSTM':\n",
        "      enc_lstm = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      enc_outputs, state_h, state_c = enc_lstm(enc_outputs)\n",
        "      enc_states = [state_h, state_c]\n",
        "\n",
        "      # Adding a LSTM layer with hidden_size internal units.\n",
        "      for i in range( 2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "\n",
        "        enc_lstm = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        enc_outputs, state_h, state_c = enc_lstm(enc_outputs,initial_state = enc_states)\n",
        "        enc_states = [state_h, state_c]\n",
        "\n",
        "    elif self.cell_type == 'GRU':\n",
        "      enc_gru = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      enc_outputs, state_h = enc_gru(enc_outputs)\n",
        "      enc_states = [state_h]\n",
        "      # Adding a GRU layer with hidden_size internal units.\n",
        "      for i in range(2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "        enc_gru = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        enc_outputs, state_h = enc_gru(enc_outputs, initial_state = enc_states)\n",
        "        enc_states = [state_h]  \n",
        "\n",
        "    elif self.cell_type == 'RNN':\n",
        "      enc_rnn = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      enc_outputs, state_h = enc_rnn(enc_outputs)\n",
        "      enc_states = [state_h]\n",
        "      # Adding a RNN layer with hidden_size internal units.\n",
        "      for i in range(2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "        enc_rnn = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        enc_outputs, state_h = enc_rnn(enc_outputs, initial_state = enc_states)\n",
        "        enc_states = [state_h]  \n",
        "\n",
        "    # The 'enc_states' are fixed as initial state fro decodeer inputs.\n",
        "    dec_inputs = Input(shape=(None,), name = 'Dec_inputs')\n",
        "    dec_emb_layer = Embedding(no_dec_tokens, self.hidden_size, mask_zero = True, name = 'Dec_emb')\n",
        "    dec_emb = dec_emb_layer(dec_inputs)\n",
        "    dec_outputs = dec_emb\n",
        "    if self.cell_type == 'LSTM':\n",
        "      dec_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      dec_outputs, _, _ = dec_lstm(dec_outputs, initial_state = enc_states)\n",
        "      \n",
        "      for i in range(2, self.num_dec +1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "\n",
        "        dec_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        dec_outputs, _, _ = dec_lstm(dec_outputs, initial_state = enc_states)\n",
        "\n",
        "    elif self.cell_type == 'GRU':\n",
        "      dec_gru = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      dec_outputs, _ = dec_gru(dec_outputs, initial_state = enc_states)\n",
        "\n",
        "      for i in range(2, self.num_dec+1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "        dec_gru = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        dec_outputs, _ = dec_gru(dec_outputs, initial_state = enc_states)\n",
        "\n",
        "    elif self.cell_type == 'RNN':\n",
        "      dec_rnn = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      dec_outputs, _ = dec_rnn(dec_outputs, initial_state = enc_states)\n",
        "\n",
        "      for i in range(2, self.num_dec+1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "        dec_rnn = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        dec_outputs, _ = dec_rnn(dec_outputs, initial_state = enc_states)\n",
        "\n",
        "    decoder_dense = Dense(no_dec_tokens, activation='softmax', name = 'dense')\n",
        "    dec_outputs = decoder_dense(dec_outputs)\n",
        "\n",
        "    # Define the model that takes encoder and decoder input \n",
        "    # to output dec_outputs\n",
        "    model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Defining the optimizer and loss function .\n",
        "    optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy'])\n",
        "  \n",
        "    model.fit(\n",
        "        [enc_input_data, dec_input_data],\n",
        "        dec_target_data,\n",
        "        batch_size=self.batch_size,\n",
        "        epochs=self.epochs\n",
        "        )\n",
        "    \n",
        "    encoder_model,decoder_model = self.inference_model(model)\n",
        "    data_list = [[\"SNO\", \"Input Data\", \"Target Data\", \"Predicted Data\"]]\n",
        " \n",
        "    global_total = 0\n",
        "    global_correct = 0\n",
        "    for i in range(len(val_source)):\n",
        "      input_seq = x_test[i : i + 1]\n",
        "      result = self.decode_sequence(encoder_model,decoder_model,input_seq)\n",
        "      target = y_test[i]\n",
        "      target = target[1:len(target)-1]\n",
        "      result = result[0:len(result)-1]\n",
        "      dlist = [i+1, val_input_lexicons[i], target, result]\n",
        "      data_list.append(dlist)\n",
        "\n",
        "      if result.strip() == target.strip():\n",
        "        global_correct = global_correct + 1\n",
        "      \n",
        "      global_total = global_total + 1\n",
        "      accuracy_epoch = global_correct/global_total\n",
        "    with open('predictions_vanilla.tsv', 'w', newline='') as file:\n",
        "      writer = csv.writer(file, delimiter='\\t')\n",
        "      writer.writerows(data_list)\n",
        "    val_accuracy = global_correct/global_total\n",
        "    print(val_accuracy)\n",
        "    \n",
        "  def inference_model(self,model):\n",
        "    enc_inputs = model.input[0] \n",
        "    if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "      enc_outputs, state_h_enc = model.get_layer('Enc_hidden_'+ str(self.num_enc)).output\n",
        "      enc_states = [state_h_enc]\n",
        "      encoder_model = Model(enc_inputs, enc_states)\n",
        "\n",
        "      dec_inputs = model.input[1]  # input_1\n",
        "      dec_outputs = model.get_layer('Dec_emb')(dec_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "\n",
        "      for i in range(1,self.num_dec +1):\n",
        "        decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "        curr_states_inputs = [decoder_state_input_h]\n",
        "        decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "        dec_outputs, state_h_dec = decoder(dec_outputs, initial_state=curr_states_inputs)\n",
        "\n",
        "        decoder_states += [state_h_dec]\n",
        "        decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "    elif self.cell_type == 'LSTM':\n",
        "      enc_outputs, state_h_enc, state_c_enc = model.get_layer('Enc_hidden_'+ str(self.num_enc)).output  # lstm_1\n",
        "      enc_states = [state_h_enc, state_c_enc]\n",
        "      encoder_model = Model(enc_inputs, enc_states)\n",
        "\n",
        "      dec_inputs = model.input[1]  # input_1\n",
        "      dec_outputs = model.get_layer('Dec_emb')(dec_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "\n",
        "      for i in range(1,self.num_dec +1):\n",
        "        decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "        decoder_state_input_c = keras.Input(shape=(self.hidden_size,))\n",
        "        curr_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "        dec_outputs, state_h_dec, state_c_dec = decoder(dec_outputs, initial_state=curr_states_inputs)\n",
        "\n",
        "        decoder_states += [state_h_dec, state_c_dec]\n",
        "        decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "\n",
        "    decoder_dense = model.get_layer('dense')\n",
        "    dec_outputs = decoder_dense(dec_outputs)\n",
        "    decoder_model = Model([dec_inputs] + decoder_states_inputs, [dec_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model,decoder_model\n",
        "\n",
        "  def decode_sequence(self,encoder_model,decoder_model,input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = [encoder_model.predict(input_seq)] * self.num_dec\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # The first character of target sequence is populated with the start character.\n",
        "    target_seq[0, 0] = target_token_index['B']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    while not stop_condition:\n",
        "        if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "          dummy = decoder_model.predict([target_seq] + [states_value])\n",
        "          output_tokens, states_value = dummy[0],dummy[1:]\n",
        "          \n",
        "        elif self.cell_type == 'LSTM':  \n",
        "          dummy = decoder_model.predict([target_seq] + states_value)\n",
        "          output_tokens, states_value = dummy[0],dummy[1:]\n",
        "        \n",
        "        #print(output_tokens[0,:,:])\n",
        "        if self.pred_type == 'greedy':\n",
        "          beam_w = 1\n",
        "        elif self.pred_type == 'beam_search':\n",
        "          beam_w = self.beam_width\n",
        "        sampled_token_index = self.beam_search_decoder(output_tokens[0,:,:], beam_w)\n",
        "        sampled_token_index = sampled_token_index[beam_w-1][0]\n",
        "\n",
        "        # Token sampling\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit when reaches max length or stop character is encountered.\n",
        "        if sampled_char == 'E' or len(decoded_sentence) > dec_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "\n",
        "    return decoded_sentence\n",
        "  \n",
        "  def beam_search_decoder(self,data, k):\n",
        "    # In beam search decoding we do not need to start with random states; instead,\n",
        "    # we start with the k most likely words as the first step in the sequence.\n",
        "    sequences = [[list(), 0.0]]\n",
        "    for row in data:\n",
        "      all_candidates = list()\n",
        "      # expand each current candidate\n",
        "      for i in range(len(sequences)):\n",
        "        seq, score = sequences[i]\n",
        "        for j in range(len(row)):\n",
        "          candidate = [seq + [j], score - log(row[j])]\n",
        "          all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "      ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "      # select k best\n",
        "      sequences = ordered[:k]\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpH2nJGdCcZ0"
      },
      "source": [
        "## Sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "y6v4kKgONH4q"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes', \n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "\n",
        "        'dropout': {\n",
        "            'values': [0.0, 0.1, 0.2]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64, 128]\n",
        "        },\n",
        "        'in_emb': {\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'num_enc': {\n",
        "            'values': [1, 2, 3]\n",
        "        },\n",
        "        'num_dec': {\n",
        "            'values': [1, 2, 3]\n",
        "        },\n",
        "        'hidden_size':{\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            'values': ['RNN', 'GRU', 'LSTM']\n",
        "        },\n",
        "        'dec_search': {\n",
        "            'values': ['beam_search', 'greedy']\n",
        "        },\n",
        "        'beam_width':{\n",
        "            'values': [3,5]\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "dC0C9Ol8QcW5"
      },
      "outputs": [],
      "source": [
        "# Initialize a new sweep\n",
        "#sweep_id = wandb.sweep(sweep_config, entity=\"girishrongali\", project=\"assignment3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BEST Parameters:"
      ],
      "metadata": {
        "id": "vs9Y0ezv-qCV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "5QeYGKXmlKHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c94fa0-24a9-40cd-e78d-f51f841ef4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Enc_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Enc_emb (Embedding)            (None, None, 128)    3456        ['Enc_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " Dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Enc_hidden_1 (LSTM)            [(None, None, 128),  131584      ['Enc_emb[0][0]']                \n",
            "                                 (None, 128),                                                     \n",
            "                                 (None, 128)]                                                     \n",
            "                                                                                                  \n",
            " Dec_emb (Embedding)            (None, None, 128)    8448        ['Dec_inputs[0][0]']             \n",
            "                                                                                                  \n",
            " Enc_hidden_2 (LSTM)            [(None, None, 128),  131584      ['Enc_hidden_1[0][0]',           \n",
            "                                 (None, 128),                     'Enc_hidden_1[0][1]',           \n",
            "                                 (None, 128)]                     'Enc_hidden_1[0][2]']           \n",
            "                                                                                                  \n",
            " Dec_hidden_1 (LSTM)            [(None, None, 128),  131584      ['Dec_emb[0][0]',                \n",
            "                                 (None, 128),                     'Enc_hidden_2[0][1]',           \n",
            "                                 (None, 128)]                     'Enc_hidden_2[0][2]']           \n",
            "                                                                                                  \n",
            " Dec_hidden_2 (LSTM)            [(None, None, 128),  131584      ['Dec_hidden_1[0][0]',           \n",
            "                                 (None, 128),                     'Enc_hidden_2[0][1]',           \n",
            "                                 (None, 128)]                     'Enc_hidden_2[0][2]']           \n",
            "                                                                                                  \n",
            " Dec_hidden_3 (LSTM)            [(None, None, 128),  131584      ['Dec_hidden_2[0][0]',           \n",
            "                                 (None, 128),                     'Enc_hidden_2[0][1]',           \n",
            "                                 (None, 128)]                     'Enc_hidden_2[0][2]']           \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 66)     8514        ['Dec_hidden_3[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 678,338\n",
            "Trainable params: 678,338\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "458/458 [==============================] - 56s 64ms/step - loss: 1.1218 - accuracy: 0.3219\n",
            "Epoch 2/20\n",
            "458/458 [==============================] - 29s 63ms/step - loss: 0.6296 - accuracy: 0.6065\n",
            "Epoch 3/20\n",
            "458/458 [==============================] - 29s 63ms/step - loss: 0.3790 - accuracy: 0.7533\n",
            "Epoch 4/20\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.2673 - accuracy: 0.8234\n",
            "Epoch 5/20\n",
            "458/458 [==============================] - 29s 63ms/step - loss: 0.2069 - accuracy: 0.8639\n",
            "Epoch 6/20\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.1705 - accuracy: 0.8871\n",
            "Epoch 7/20\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.1459 - accuracy: 0.9029\n",
            "Epoch 8/20\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.1282 - accuracy: 0.9143\n",
            "Epoch 9/20\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.1144 - accuracy: 0.9237\n",
            "Epoch 10/20\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.1037 - accuracy: 0.9306\n",
            "Epoch 11/20\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.0950 - accuracy: 0.9364\n",
            "Epoch 12/20\n",
            "458/458 [==============================] - 29s 63ms/step - loss: 0.0872 - accuracy: 0.9416\n",
            "Epoch 13/20\n",
            "458/458 [==============================] - 29s 63ms/step - loss: 0.0803 - accuracy: 0.9462\n",
            "Epoch 14/20\n",
            "458/458 [==============================] - 29s 63ms/step - loss: 0.0744 - accuracy: 0.9499\n",
            "Epoch 15/20\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.0696 - accuracy: 0.9530\n",
            "Epoch 16/20\n",
            "458/458 [==============================] - 29s 63ms/step - loss: 0.0655 - accuracy: 0.9557\n",
            "Epoch 17/20\n",
            "458/458 [==============================] - 29s 63ms/step - loss: 0.0611 - accuracy: 0.9589\n",
            "Epoch 18/20\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.0577 - accuracy: 0.9609\n",
            "Epoch 19/20\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.0543 - accuracy: 0.9631\n",
            "Epoch 20/20\n",
            "458/458 [==============================] - 29s 64ms/step - loss: 0.0512 - accuracy: 0.9649\n"
          ]
        }
      ],
      "source": [
        "best_batch_size = 64\n",
        "best_beam_width = 5\n",
        "best_cell_type = 'GRU'\n",
        "best_dec_search = 'beam_search'\n",
        "best_dropout = 0.2\n",
        "best_epochs = 20\n",
        "best_hidden_size = 128\n",
        "best_in_emb = 128\n",
        "best_learning_rate = 0.001\n",
        "best_num_dec = 3\n",
        "best_num_enc = 3\n",
        "  \n",
        "model_rnn = GJRNN(cell_type = best_cell_type, in_emb = best_in_emb, hidden_size=best_hidden_size,\n",
        "                learning_rate= best_learning_rate, dropout=best_dropout,pred_type = best_dec_search,epochs = best_epochs,\n",
        "                batch_size = best_batch_size, beam_width = best_beam_width, num_enc = best_num_enc, num_dec = best_num_dec)\n",
        "  \n",
        "model_rnn.model_build(enc_input_data,dec_input_data,dec_target_data,x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "AHqwHj8ukHYI"
      },
      "outputs": [],
      "source": [
        "#sweep_id = wandb.sweep(sweep_config, entity=\"girishrongali\", project=\"assignment3\")\n",
        "#wandb.agent(sweep_id, lambda : train_sweep())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"predictions_vanilla.tsv\")"
      ],
      "metadata": {
        "id": "rgcUR46XulLS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d2168021-0f25-4e61-c28b-df95395a6c95"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0eb891f2-430d-4f56-ad7b-db49eaf83f6b\", \"predictions_vanilla.tsv\", 355602)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "TESTING_Transliteration_without_attention_FINAL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}