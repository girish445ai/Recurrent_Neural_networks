{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/girish445ai/Recurrent_Neural_networks/blob/main/Transliteration_without_attention_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTK4IFUhLr8N"
      },
      "source": [
        "### Importing Libraries "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apeRGrBs6yH8"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import numpy as np\n",
        "import tensorflow \n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, GRU, Dropout, SimpleRNN\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from math import log\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "from math import log1p "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NsYPEEpM5cU",
        "outputId": "7a18df9e-6505-4791-8f20-0d2bfdb77b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 46.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 44.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install wandb -q\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6lJCkKMLwDe"
      },
      "source": [
        "### Unzipping the dataset\n",
        "\n",
        "Lexicons for Latin-Telugu are taken from Google's Dakshina dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhpFjBqec5sk",
        "outputId": "4748b9b2-3b1e-4669-9e16-bd04dfcce32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-04 17:43:39--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.219.128, 209.85.146.128, 142.250.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.219.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   225MB/s    in 9.3s    \n",
            "\n",
            "2022-05-04 17:43:49 (206 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Downloading dakshina dataset\n",
        "!yes | wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8V3WTrkfF66"
      },
      "outputs": [],
      "source": [
        "# Unzipping dataset\n",
        "!yes | tar xopf dakshina_dataset_v1.0.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9IlHm7CfO_I",
        "outputId": "597356c6-4da1-495a-e5ce-c3bb5324d25a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "te.translit.sampled.dev.tsv   te.translit.sampled.train.tsv\n",
            "te.translit.sampled.test.tsv\n"
          ]
        }
      ],
      "source": [
        "# The folder containing the datasets to be used in this program\n",
        "!ls dakshina_dataset_v1.0/te/lexicons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x95CxiMFl54"
      },
      "outputs": [],
      "source": [
        "print_data = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPoGXywuL1kD"
      },
      "source": [
        "## Reading the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBzvL2e20Sx-",
        "outputId": "65ec8163-7b50-4922-828a-35cdc2eb640a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  58550\n",
            "Number of validation samples:  5683\n",
            "Number of testing samples:  5683\n"
          ]
        }
      ],
      "source": [
        "train_path = \"./dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.train.tsv\"\n",
        "dev_path = \"./dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\"\n",
        "test_path = \"./dakshina_dataset_v1.0/te/lexicons/te.translit.sampled.dev.tsv\"\n",
        "\n",
        "def reading_data(corpus_file):\n",
        "  # function reads the raw text of words and returns native versions of words\n",
        "  telugu_words = []\n",
        "  latin_words = []\n",
        "  with io.open(corpus_file, encoding ='utf-8') as f:\n",
        "    for line in f:\n",
        "      if '\\t' not in line:\n",
        "        continue\n",
        "      tokens = line.rstrip().split(\"\\t\")\n",
        "      latin_words.append(tokens[1])\n",
        "      telugu_words.append(tokens[0])\n",
        "  return latin_words, telugu_words\n",
        "\n",
        "train_source, train_target = reading_data(train_path)\n",
        "val_source, val_target = reading_data(dev_path)\n",
        "test_source, test_target = reading_data(test_path)\n",
        "\n",
        "print(\"Number of training samples: \", len(train_source))\n",
        "print(\"Number of validation samples: \", len(val_source))\n",
        "print(\"Number of testing samples: \", len(test_source))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mC2ICFKRUZA",
        "outputId": "522ecd5a-3b94-4924-bacb-b56f8be78035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 58550\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 25\n",
            "Max sequence length for outputs: 22\n",
            "Max sequence length for val inputs: 21\n",
            "Max sequence length for val outputs: 21\n",
            "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "[' ', 'B', 'E', 'ం', 'ః', 'అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ', 'క', 'ఖ', 'గ', 'ఘ', 'చ', 'ఛ', 'జ', 'ఝ', 'ఞ', 'ట', 'ఠ', 'డ', 'ఢ', 'ణ', 'త', 'థ', 'ద', 'ధ', 'న', 'ప', 'ఫ', 'బ', 'భ', 'మ', 'య', 'ర', 'ఱ', 'ల', 'ళ', 'వ', 'శ', 'ష', 'స', 'హ', 'ా', 'ి', 'ీ', 'ు', 'ూ', 'ృ', 'ె', 'ే', 'ై', 'ొ', 'ో', 'ౌ', '్', '\\u200c']\n"
          ]
        }
      ],
      "source": [
        "arr = np.arange(len(train_source))\n",
        "np.random.shuffle(arr)\n",
        "arr1 = np.arange(len(val_source))\n",
        "np.random.shuffle(arr1)\n",
        "\n",
        "input_chars = set()\n",
        "target_chars = set()\n",
        "input_texts_ns = []\n",
        "target_texts_ns = []\n",
        "val_input_texts_ns = []\n",
        "val_target_texts_ns = []\n",
        "\n",
        "for (input_text, target_text) in zip(train_source, train_target):\n",
        "    # \"tab\" is the \"start sequence\" characte ,\"\\n\" is \"end sequence\" character.\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    input_texts_ns.append(input_text)\n",
        "    target_texts_ns.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_chars:\n",
        "            input_chars.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_chars:\n",
        "            target_chars.add(char)\n",
        "\n",
        "for (input_text, target_text) in zip(val_source, val_target):\n",
        "    # \"tab\" is the \"start sequence\" characte ,\"\\n\" is \"end sequence\" character.\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    val_input_texts_ns.append(input_text)\n",
        "    val_target_texts_ns.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_chars:\n",
        "            input_chars.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_chars:\n",
        "            target_chars.add(char)\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "for i in range(len(train_source)):\n",
        "    input_texts.append(input_texts_ns[arr[i]])\n",
        "    target_texts.append(target_texts_ns[arr[i]])\n",
        "\n",
        "val_input_texts = []\n",
        "val_target_texts = []\n",
        "\n",
        "for i in range(len(val_source)):\n",
        "    val_input_texts.append(val_input_texts_ns[arr1[i]])\n",
        "    val_target_texts.append(val_target_texts_ns[arr1[i]])\n",
        "\n",
        "input_chars.add(\" \")\n",
        "target_chars.add(\" \")\n",
        "\n",
        "input_chars = sorted(list(input_chars))\n",
        "target_chars = sorted(list(target_chars))\n",
        "\n",
        "\n",
        "no_enc_tokens = len(input_chars)\n",
        "no_dec_tokens = len(target_chars)\n",
        "enc_seq_length = max([len(txt) for txt in input_texts])\n",
        "dec_seq_length = max([len(txt) for txt in target_texts])\n",
        "val_max_encoder_seq_length = max([len(txt) for txt in val_input_texts])\n",
        "val_max_decoder_seq_length = max([len(txt) for txt in val_target_texts])\n",
        "\n",
        "\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", no_enc_tokens)\n",
        "print(\"Number of unique output tokens:\", no_dec_tokens)\n",
        "print(\"Max sequence length for inputs:\", enc_seq_length)\n",
        "print(\"Max sequence length for outputs:\", dec_seq_length)\n",
        "print(\"Max sequence length for val inputs:\", val_max_encoder_seq_length)\n",
        "print(\"Max sequence length for val outputs:\", val_max_decoder_seq_length)\n",
        "\n",
        "print(input_chars)\n",
        "print(target_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f7y6C51r1QI",
        "outputId": "74b05d1e-6ae2-428a-f986-bf489a45bbaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ghana', 'kodukaina', 'maraninchaaru', 'bhaaryaabhartalu', 'labhyatanu', 'nirnayamu', 'mathru']\n",
            "['BఘనాE', 'BకొడుకైనE', 'BమరణించారుE', 'Bభార్యాభర్తలుE', 'Bలభ్యతనుE', 'Bనిర్ణయముE', 'BమాతృE']\n"
          ]
        }
      ],
      "source": [
        "print(input_texts[123:130])\n",
        "print(target_texts[123:130])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwPNb93PRwHT"
      },
      "source": [
        "**Training** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtHaPQPw6VuP",
        "outputId": "1a5b52d3-4d98-403a-8ad4-a4a07e87a3b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "{' ': 0, 'B': 1, 'E': 2, 'ం': 3, 'ః': 4, 'అ': 5, 'ఆ': 6, 'ఇ': 7, 'ఈ': 8, 'ఉ': 9, 'ఊ': 10, 'ఋ': 11, 'ఎ': 12, 'ఏ': 13, 'ఐ': 14, 'ఒ': 15, 'ఓ': 16, 'ఔ': 17, 'క': 18, 'ఖ': 19, 'గ': 20, 'ఘ': 21, 'చ': 22, 'ఛ': 23, 'జ': 24, 'ఝ': 25, 'ఞ': 26, 'ట': 27, 'ఠ': 28, 'డ': 29, 'ఢ': 30, 'ణ': 31, 'త': 32, 'థ': 33, 'ద': 34, 'ధ': 35, 'న': 36, 'ప': 37, 'ఫ': 38, 'బ': 39, 'భ': 40, 'మ': 41, 'య': 42, 'ర': 43, 'ఱ': 44, 'ల': 45, 'ళ': 46, 'వ': 47, 'శ': 48, 'ష': 49, 'స': 50, 'హ': 51, 'ా': 52, 'ి': 53, 'ీ': 54, 'ు': 55, 'ూ': 56, 'ృ': 57, 'ె': 58, 'ే': 59, 'ై': 60, 'ొ': 61, 'ో': 62, 'ౌ': 63, '్': 64, '\\u200c': 65}\n"
          ]
        }
      ],
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(input_chars)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_chars)])\n",
        "print(input_token_index)\n",
        "print(target_token_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3i7ykYbtQ2K"
      },
      "outputs": [],
      "source": [
        "# Encoder Input Sequences are padded to a maximum length of MAX encoder SeqLen characters. \n",
        "enc_input_data = np.zeros(\n",
        "    (len(input_texts), enc_seq_length), dtype=\"float32\"\n",
        ")\n",
        "dec_input_data = np.zeros(\n",
        "    (len(input_texts), dec_seq_length), dtype=\"float32\"\n",
        ")\n",
        "dec_target_data = np.zeros(\n",
        "    (len(input_texts), dec_seq_length, no_dec_tokens), dtype=\"float32\"\n",
        ")\n",
        "#Decoder Target Sequences are Padded to a maximum length of max_decoder SeqLen characters with a vocabulary of sizeofTeluguVocab different characters. \n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        enc_input_data[i, t] = input_token_index[char]\n",
        "    enc_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        # dec_target_data is ahead of dec_input_data by one timestep\n",
        "        dec_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # dec_target_data will not include the start character.\n",
        "            dec_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    dec_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    dec_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "\n",
        "val_enc_input_data = np.zeros(\n",
        "    (len(input_texts), val_max_encoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_dec_input_data = np.zeros(\n",
        "    (len(input_texts), val_max_decoder_seq_length), dtype=\"float32\"\n",
        ")\n",
        "val_dec_target_data = np.zeros(\n",
        "    (len(input_texts), val_max_decoder_seq_length, no_dec_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(val_input_texts, val_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        val_enc_input_data[i, t] = input_token_index[char]\n",
        "    val_enc_input_data[i, t + 1 :] = input_token_index[\" \"]\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "        # dec_target_data is ahead of decoder_input_data by one timestep\n",
        "        val_dec_input_data[i, t] = target_token_index[char]\n",
        "        if t > 0:\n",
        "            # dec_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            val_dec_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    val_dec_input_data[i, t + 1: ] = target_token_index[\" \"]\n",
        "    val_dec_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOUdwNGG39qb",
        "outputId": "92f44d6b-8701-4dc5-da80-700533647d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: ' ', 1: 'B', 2: 'E', 3: 'ం', 4: 'ః', 5: 'అ', 6: 'ఆ', 7: 'ఇ', 8: 'ఈ', 9: 'ఉ', 10: 'ఊ', 11: 'ఋ', 12: 'ఎ', 13: 'ఏ', 14: 'ఐ', 15: 'ఒ', 16: 'ఓ', 17: 'ఔ', 18: 'క', 19: 'ఖ', 20: 'గ', 21: 'ఘ', 22: 'చ', 23: 'ఛ', 24: 'జ', 25: 'ఝ', 26: 'ఞ', 27: 'ట', 28: 'ఠ', 29: 'డ', 30: 'ఢ', 31: 'ణ', 32: 'త', 33: 'థ', 34: 'ద', 35: 'ధ', 36: 'న', 37: 'ప', 38: 'ఫ', 39: 'బ', 40: 'భ', 41: 'మ', 42: 'య', 43: 'ర', 44: 'ఱ', 45: 'ల', 46: 'ళ', 47: 'వ', 48: 'శ', 49: 'ష', 50: 'స', 51: 'హ', 52: 'ా', 53: 'ి', 54: 'ీ', 55: 'ు', 56: 'ూ', 57: 'ృ', 58: 'ె', 59: 'ే', 60: 'ై', 61: 'ొ', 62: 'ో', 63: 'ౌ', 64: '్', 65: '\\u200c'}\n"
          ]
        }
      ],
      "source": [
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "print(reverse_target_char_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjskNaet7bgj",
        "outputId": "f5f09ef7-180d-4e1b-d3fa-4fb2006e99bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.  9. 18. 14.  1. 25.  9. 14.  3.  8. 21. 11. 15. 14. 14.  1.  1. 18.\n",
            " 21.  0.  0.  0.  0.  0.  0.]\n",
            "[ 1. 36. 53. 43. 64. 31. 42. 53.  3. 22. 55. 18. 61. 36. 64. 36. 52. 43.\n",
            " 55.  2.  0.  0.]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(enc_input_data[1])\n",
        "print(dec_input_data[1])\n",
        "print(dec_target_data[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui9pFxRvSdeT"
      },
      "source": [
        "For Validation and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVGsHbTJn807",
        "outputId": "a4b6e7d2-f068-46ab-9803-99fc8174afd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1. 20. 63. 32. 41.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "print(val_dec_input_data[26])\n",
        "print(val_dec_target_data[26])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7q6xNssrRwb"
      },
      "outputs": [],
      "source": [
        "x_test = val_enc_input_data\n",
        "y_test = val_target_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL TRAINING "
      ],
      "metadata": {
        "id": "fsl1l9XK_jm0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvI1fI7UOPMT"
      },
      "outputs": [],
      "source": [
        "class MyRNN(object):\n",
        "  def __init__(self,cell_type = 'RNN',in_emb = 32, hidden_size=32, learning_rate= 1e-3, \n",
        "               dropout=0.4,pred_type ='greedy',epochs = 10, batch_size = 32,beam_width = 5,\n",
        "               num_enc = 1,num_dec = 1):\n",
        "    \n",
        "    self.cell_type = cell_type\n",
        "    self.in_emb = in_emb\n",
        "    self.hidden_size = hidden_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.dropout = dropout\n",
        "    self.pred_type = pred_type\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.beam_width = beam_width\n",
        "    self.num_enc = num_enc\n",
        "    self.num_dec = num_dec\n",
        "\n",
        "  def build_fit(self,enc_input_data,dec_input_data,dec_target_data,x_test, y_test):\n",
        "    enc_inputs = Input(shape=(None, ),name = 'Enc_inputs')\n",
        "\n",
        "    # Add an Embedding layer expecting input vocab of size no_enc_tokens, and\n",
        "    # output embedding dimension of size in_enc.\n",
        "    enc_emb =  Embedding(no_enc_tokens, self.in_emb , mask_zero = True,name = 'Enc_emb')(enc_inputs)\n",
        "\n",
        "    enc_outputs = enc_emb\n",
        "    if self.cell_type == 'LSTM':\n",
        "      encoder_lstm = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      enc_outputs, state_h, state_c = encoder_lstm(enc_outputs)\n",
        "      encoder_states = [state_h, state_c]\n",
        "\n",
        "      # Add a LSTM layer with hidden_size internal units.\n",
        "      for i in range( 2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "\n",
        "        encoder_lstm = LSTM(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        enc_outputs, state_h, state_c = encoder_lstm(enc_outputs,initial_state = encoder_states)\n",
        "        encoder_states = [state_h, state_c]\n",
        "\n",
        "    elif self.cell_type == 'GRU':\n",
        "      encoder_gru = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      enc_outputs, state_h = encoder_gru(enc_outputs)\n",
        "      encoder_states = [state_h]\n",
        "\n",
        "      for i in range(2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "        encoder_gru = GRU(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        enc_outputs, state_h = encoder_gru(enc_outputs, initial_state = encoder_states)\n",
        "        encoder_states = [state_h]  \n",
        "\n",
        "    elif self.cell_type == 'RNN':\n",
        "      encoder_rnn = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=\"Enc_hidden_1\")\n",
        "      enc_outputs, state_h = encoder_rnn(enc_outputs)\n",
        "      encoder_states = [state_h]\n",
        "\n",
        "      for i in range(2, self.num_enc +1):\n",
        "        layer_name = ('Enc_hidden_%d') %i\n",
        "        encoder_rnn = SimpleRNN(self.hidden_size, return_state=True,dropout = self.dropout, return_sequences=True, name=layer_name)\n",
        "        enc_outputs, state_h = encoder_rnn(enc_outputs, initial_state = encoder_states)\n",
        "        encoder_states = [state_h]  \n",
        "\n",
        "    # Set up the decoder, using `encoder_states` as initial state.\n",
        "    dec_inputs = Input(shape=(None,), name = 'Dec_inputs')\n",
        "    dec_emb_layer = Embedding(no_dec_tokens, self.hidden_size, mask_zero = True, name = 'Dec_emb')\n",
        "    dec_emb = dec_emb_layer(dec_inputs)\n",
        "    # We set up our decoder to return full output sequences,\n",
        "    # and to return internal states as well. We don't use the\n",
        "    # return states in the training model, but we will use them in inference.\n",
        "    dec_outputs = dec_emb\n",
        "    if self.cell_type == 'LSTM':\n",
        "      decoder_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      dec_outputs, _, _ = decoder_lstm(dec_outputs, initial_state = encoder_states)\n",
        "      \n",
        "      for i in range(2, self.num_dec +1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "\n",
        "        decoder_lstm = LSTM(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        dec_outputs, _, _ = decoder_lstm(dec_outputs, initial_state = encoder_states)\n",
        "\n",
        "    elif self.cell_type == 'GRU':\n",
        "      decoder_gru = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      dec_outputs, _ = decoder_gru(dec_outputs, initial_state = encoder_states)\n",
        "\n",
        "      for i in range(2, self.num_dec+1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "        decoder_gru = GRU(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        dec_outputs, _ = decoder_gru(dec_outputs, initial_state = encoder_states)\n",
        "\n",
        "    elif self.cell_type == 'RNN':\n",
        "      decoder_rnn = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=\"Dec_hidden_1\")\n",
        "      dec_outputs, _ = decoder_rnn(dec_outputs, initial_state = encoder_states)\n",
        "\n",
        "      for i in range(2, self.num_dec+1):\n",
        "        layer_name = ('Dec_hidden_%d') %i\n",
        "        decoder_rnn = SimpleRNN(self.hidden_size, return_sequences=True, return_state=True,dropout = self.dropout, name=layer_name)\n",
        "        dec_outputs, _ = decoder_rnn(dec_outputs, initial_state = encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(no_dec_tokens, activation='softmax', name = 'dense')\n",
        "    dec_outputs = decoder_dense(dec_outputs)\n",
        "\n",
        "    # Define the model that takes encoder and decoder input \n",
        "    # to output dec_outputs\n",
        "    model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    # Define the optimizer\n",
        "    optimizer = Adam(lr=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, metrics=['accuracy'])\n",
        "  \n",
        "    model.fit(\n",
        "        [enc_input_data, dec_input_data],\n",
        "        dec_target_data,\n",
        "        batch_size=self.batch_size,\n",
        "        epochs=self.epochs,\n",
        "        callbacks = [WandbCallback()]\n",
        "        )\n",
        "    \n",
        "    encoder_model,decoder_model = self.inference_model(model)\n",
        " \n",
        "    global_total = 0\n",
        "    global_correct = 0\n",
        "    for i in range(len(val_source)):\n",
        "      #input_seq = val_enc_input_data[i : i + 1]\n",
        "      input_seq = x_test[i : i + 1]\n",
        "      result = self.decode_sequence(encoder_model,decoder_model,input_seq)\n",
        "      #target = val_target_texts[i]\n",
        "      target = y_test[i]\n",
        "      target = target[1:len(target)-1]\n",
        "      result = result[0:len(result)-1]\n",
        "      #print(\"Target: %s \\n Result: %s\" % (target, result))\n",
        "\n",
        "      if result.strip() == target.strip():\n",
        "        global_correct = global_correct + 1\n",
        "      \n",
        "      global_total = global_total + 1\n",
        "      accuracy_epoch = global_correct/global_total\n",
        "      if global_total % 50 == 0:\n",
        "        wandb.log({'epoch_accuracy' : accuracy_epoch})\n",
        "      #print(\"Accuracy: %s\" % (accuracy_epoch))\n",
        "    \n",
        "    val_accuracy = global_correct/global_total\n",
        "    #print(val_accuracy)\n",
        "\n",
        "\n",
        "    wandb.log({'val_accuracy' : val_accuracy})\n",
        "    \n",
        "  def inference_model(self,model):\n",
        "    enc_inputs = model.input[0]  # input_1\n",
        "    if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "      enc_outputs, state_h_enc = model.get_layer('Enc_hidden_'+ str(self.num_enc)).output\n",
        "      encoder_states = [state_h_enc]\n",
        "      encoder_model = Model(enc_inputs, encoder_states)\n",
        "\n",
        "      dec_inputs = model.input[1]  # input_1\n",
        "      dec_outputs = model.get_layer('Dec_emb')(dec_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "\n",
        "      for i in range(1,self.num_dec +1):\n",
        "        decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "        curr_states_inputs = [decoder_state_input_h]\n",
        "        decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "        dec_outputs, state_h_dec = decoder(dec_outputs, initial_state=curr_states_inputs)\n",
        "\n",
        "        decoder_states += [state_h_dec]\n",
        "        decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "    elif self.cell_type == 'LSTM':\n",
        "      enc_outputs, state_h_enc, state_c_enc = model.get_layer('Enc_hidden_'+ str(self.num_enc)).output  # lstm_1\n",
        "      encoder_states = [state_h_enc, state_c_enc]\n",
        "      encoder_model = Model(enc_inputs, encoder_states)\n",
        "\n",
        "      dec_inputs = model.input[1]  # input_1\n",
        "      dec_outputs = model.get_layer('Dec_emb')(dec_inputs)\n",
        "      decoder_states_inputs = []\n",
        "      decoder_states = []\n",
        "\n",
        "      for i in range(1,self.num_dec +1):\n",
        "        decoder_state_input_h = keras.Input(shape=(self.hidden_size,))\n",
        "        decoder_state_input_c = keras.Input(shape=(self.hidden_size,))\n",
        "        curr_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "        decoder = model.get_layer('Dec_hidden_'+ str(i))\n",
        "        dec_outputs, state_h_dec, state_c_dec = decoder(dec_outputs, initial_state=curr_states_inputs)\n",
        "\n",
        "        decoder_states += [state_h_dec, state_c_dec]\n",
        "        decoder_states_inputs += curr_states_inputs\n",
        "\n",
        "\n",
        "    decoder_dense = model.get_layer('dense')\n",
        "    dec_outputs = decoder_dense(dec_outputs)\n",
        "    decoder_model = Model([dec_inputs] + decoder_states_inputs, [dec_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model,decoder_model\n",
        "\n",
        "  def decode_sequence(self,encoder_model,decoder_model,input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = [encoder_model.predict(input_seq)] * self.num_dec\n",
        "    \n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = target_token_index['B']\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    while not stop_condition:\n",
        "        if self.cell_type == 'RNN' or self.cell_type == 'GRU':\n",
        "          dummy = decoder_model.predict([target_seq] + [states_value])\n",
        "          output_tokens, states_value = dummy[0],dummy[1:]\n",
        "          \n",
        "        elif self.cell_type == 'LSTM':  \n",
        "          dummy = decoder_model.predict([target_seq] + states_value)\n",
        "          output_tokens, states_value = dummy[0],dummy[1:]\n",
        "        \n",
        "        #print(output_tokens[0,:,:])\n",
        "        if self.pred_type == 'greedy':\n",
        "          beam_w = 1\n",
        "        elif self.pred_type == 'beam_search':\n",
        "          beam_w = self.beam_width\n",
        "        sampled_token_index = self.beam_search_decoder(output_tokens[0,:,:], beam_w)\n",
        "        sampled_token_index = sampled_token_index[beam_w-1][0]\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit when reaches max length or stop character is encountered.\n",
        "        if sampled_char == 'E' or len(decoded_sentence) > dec_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update state\n",
        "\n",
        "    return decoded_sentence\n",
        "  \n",
        "  def beam_search_decoder(self,data, k):\n",
        "    sequences = [[list(), 0.0]]\n",
        "    # read each step in sequence\n",
        "    for row in data:\n",
        "      all_candidates = list()\n",
        "      # expand each current candidate\n",
        "      for i in range(len(sequences)):\n",
        "        seq, score = sequences[i]\n",
        "        for j in range(len(row)):\n",
        "          candidate = [seq + [j], score - log(row[j])]\n",
        "          all_candidates.append(candidate)\n",
        "      # order all candidates by score\n",
        "      ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
        "      # select k best\n",
        "      sequences = ordered[:k]\n",
        "    return sequences"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Transliteration_without_attention_FINAL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}