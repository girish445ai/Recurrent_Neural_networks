{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention final with viz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0TVuCaO45AXesp4zp2QBi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "66547759188441c3aea3deeded141ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a22ae25cfdb4f98a95748a598831e83",
              "IPY_MODEL_e56c903275a24762ad46f0d4fc5e9e77"
            ],
            "layout": "IPY_MODEL_47c91e040dfa4d15bdc54c01f52088bf"
          }
        },
        "9a22ae25cfdb4f98a95748a598831e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca71125cf37442c58fb137f9e88c1e62",
            "placeholder": "​",
            "style": "IPY_MODEL_d2e46bd2b31a43f0aa8c4f3c70fb8d02",
            "value": "0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "e56c903275a24762ad46f0d4fc5e9e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7181ff4308e642bfb4d699d7b1ced2fc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d99fc56c41d44409a9f7ddca38ad8b10",
            "value": 1
          }
        },
        "47c91e040dfa4d15bdc54c01f52088bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca71125cf37442c58fb137f9e88c1e62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2e46bd2b31a43f0aa8c4f3c70fb8d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7181ff4308e642bfb4d699d7b1ced2fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d99fc56c41d44409a9f7ddca38ad8b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "SwPD1KP_1gxY"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import models\n",
        "from numpy import array_equal\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, SimpleRNN, GRU\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow as tf\n",
        "import io\n",
        "\n",
        "tf.keras.backend.set_floatx('float64')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s97b-dY2PPQ",
        "outputId": "e6a97637-4dea-4f4d-8d52-384cad23c560"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.16)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb) (1.2.3)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.11)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.27)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb \n",
        "from wandb.keras import WandbCallback"
      ],
      "metadata": {
        "id": "ZkoWMPGT2TiN"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading dakshina dataset\n",
        "!yes | wget \"https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiIuOh8K1hd9",
        "outputId": "ede3719a-b7e4-4b80-d61c-24b413d92547"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-07 11:27:37--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.157.128, 142.251.8.128, 74.125.23.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.157.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2008340480 (1.9G) [application/x-tar]\n",
            "Saving to: ‘dakshina_dataset_v1.0.tar.2’\n",
            "\n",
            "dakshina_dataset_v1 100%[===================>]   1.87G   172MB/s    in 11s     \n",
            "\n",
            "2022-05-07 11:27:48 (182 MB/s) - ‘dakshina_dataset_v1.0.tar.2’ saved [2008340480/2008340480]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping dataset\n",
        "!yes | tar xopf dakshina_dataset_v1.0.tar"
      ],
      "metadata": {
        "id": "1vJk4lCO3hCa"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The folder containing the datasets to be used in this program\n",
        "!ls dakshina_dataset_v1.0/te/lexicons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4InWc9ZE3llD",
        "outputId": "ea44e3c6-ec61-405c-cf14-49bb1a547b86"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "te.translit.sampled.dev.tsv   te.translit.sampled.train.tsv\n",
            "te.translit.sampled.test.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/girish445ai/lexicons.git "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYkmhZIFMfXz",
        "outputId": "775fe987-d0c6-433b-b0c6-e10d85d70e9e"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lexicons'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"lexicon-dataset/te.translit.sampled.train.tsv\"\n",
        "dev_dir = \"lexicon-dataset/te.translit.sampled.dev.tsv\"\n",
        "test_dir = \"lexicon-dataset/te.translit.sampled.test.tsv\"\n",
        "\n",
        "\n",
        "def read_corpus(corpus_file):\n",
        "  tamil_words = []\n",
        "  latin_words = []\n",
        "  with io.open(corpus_file, encoding ='utf-8') as f:\n",
        "    for line in f:\n",
        "      if '\\t' not in line:\n",
        "        continue\n",
        "      tokens = line.rstrip().split(\"\\t\")\n",
        "      latin_words.append(tokens[1])\n",
        "      tamil_words.append(tokens[0])\n",
        "  return latin_words, tamil_words\n",
        "\n",
        "train_source, train_target = read_corpus(train_dir)\n",
        "valid_source, valid_target = read_corpus(dev_dir)\n",
        "test_source, test_target = read_corpus(test_dir)\n",
        "\n",
        "print(\"Number of training samples: \", len(train_source))\n",
        "print(\"Number of validation samples: \", len(valid_source))\n",
        "print(\"Number of testing samples: \", len(test_source))"
      ],
      "metadata": {
        "id": "aLYvf2V-1k17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08eef153-23fd-4400-d148-1704f9c6a903"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  58550\n",
            "Number of validation samples:  5683\n",
            "Number of testing samples:  5747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = np.arange(len(train_source))\n",
        "np.random.shuffle(arr)\n",
        "arr1 = np.arange(len(valid_source))\n",
        "np.random.shuffle(arr1)\n",
        "\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "input_texts_ns = []\n",
        "target_texts_ns = []\n",
        "val_input_texts_ns = []\n",
        "val_target_texts_ns = []\n",
        "\n",
        "for (input_text, target_text) in zip(train_source, train_target):\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    input_texts_ns.append(input_text)\n",
        "    target_texts_ns.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "for (input_text, target_text) in zip(valid_source, valid_target):\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"B\" + target_text + \"E\"\n",
        "    val_input_texts_ns.append(input_text)\n",
        "    val_target_texts_ns.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "      \n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "\n",
        "for i in range(len(train_source)):\n",
        "    input_texts.append(input_texts_ns[arr[i]])\n",
        "    target_texts.append(target_texts_ns[arr[i]])\n",
        "\n",
        "val_input_texts = []\n",
        "val_target_texts = []\n",
        "\n",
        "for i in range(len(valid_source)):\n",
        "    val_input_texts.append(val_input_texts_ns[arr1[i]])\n",
        "    val_target_texts.append(val_target_texts_ns[arr1[i]])\n",
        "\n",
        "input_characters.add(\" \")\n",
        "target_characters.add(\" \")\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "\n",
        "\n",
        "\n",
        "# Adding the padding character\n",
        "#input_characters.append(\"P\")\n",
        "#target_characters.append(\"P\")\n",
        "\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "val_max_encoder_seq_length = max([len(txt) for txt in val_input_texts])\n",
        "val_max_decoder_seq_length = max([len(txt) for txt in val_target_texts])\n",
        "\n",
        "\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "print(\"Max sequence length for val inputs:\", val_max_encoder_seq_length)\n",
        "print(\"Max sequence length for val outputs:\", val_max_decoder_seq_length)\n",
        "\n",
        "print(input_characters)\n",
        "print(target_characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "do4mni_412Yy",
        "outputId": "57ac5ea8-8ee1-4e62-ea02-edb8d0174a9e"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 58550\n",
            "Number of unique input tokens: 27\n",
            "Number of unique output tokens: 66\n",
            "Max sequence length for inputs: 25\n",
            "Max sequence length for outputs: 22\n",
            "Max sequence length for val inputs: 21\n",
            "Max sequence length for val outputs: 21\n",
            "[' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "[' ', 'B', 'E', 'ం', 'ః', 'అ', 'ఆ', 'ఇ', 'ఈ', 'ఉ', 'ఊ', 'ఋ', 'ఎ', 'ఏ', 'ఐ', 'ఒ', 'ఓ', 'ఔ', 'క', 'ఖ', 'గ', 'ఘ', 'చ', 'ఛ', 'జ', 'ఝ', 'ఞ', 'ట', 'ఠ', 'డ', 'ఢ', 'ణ', 'త', 'థ', 'ద', 'ధ', 'న', 'ప', 'ఫ', 'బ', 'భ', 'మ', 'య', 'ర', 'ఱ', 'ల', 'ళ', 'వ', 'శ', 'ష', 'స', 'హ', 'ా', 'ి', 'ీ', 'ు', 'ూ', 'ృ', 'ె', 'ే', 'ై', 'ొ', 'ో', 'ౌ', '్', '\\u200c']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "reverse_source_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "print(input_token_index)\n",
        "print(target_token_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4ajbK5c185e",
        "outputId": "fa9fb77a-d00a-4691-faef-749af858538d"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
            "{' ': 0, 'B': 1, 'E': 2, 'ం': 3, 'ః': 4, 'అ': 5, 'ఆ': 6, 'ఇ': 7, 'ఈ': 8, 'ఉ': 9, 'ఊ': 10, 'ఋ': 11, 'ఎ': 12, 'ఏ': 13, 'ఐ': 14, 'ఒ': 15, 'ఓ': 16, 'ఔ': 17, 'క': 18, 'ఖ': 19, 'గ': 20, 'ఘ': 21, 'చ': 22, 'ఛ': 23, 'జ': 24, 'ఝ': 25, 'ఞ': 26, 'ట': 27, 'ఠ': 28, 'డ': 29, 'ఢ': 30, 'ణ': 31, 'త': 32, 'థ': 33, 'ద': 34, 'ధ': 35, 'న': 36, 'ప': 37, 'ఫ': 38, 'బ': 39, 'భ': 40, 'మ': 41, 'య': 42, 'ర': 43, 'ఱ': 44, 'ల': 45, 'ళ': 46, 'వ': 47, 'శ': 48, 'ష': 49, 'స': 50, 'హ': 51, 'ా': 52, 'ి': 53, 'ీ': 54, 'ు': 55, 'ూ': 56, 'ృ': 57, 'ె': 58, 'ే': 59, 'ై': 60, 'ొ': 61, 'ో': 62, 'ౌ': 63, '్': 64, '\\u200c': 65}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trunc_input_texts = input_texts[:68096]\n",
        "trunc_target_texts = target_texts[:68096]\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(trunc_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float64\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(trunc_input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float64\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(trunc_input_texts, trunc_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_target_data[i, t, target_token_index[char]] = 1.0\n",
        "    decoder_target_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    \n",
        "val_encoder_input_data = np.zeros(\n",
        "    (len(val_input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float64\"\n",
        ")\n",
        "val_decoder_target_data = np.zeros(\n",
        "    (len(val_target_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float64\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(val_input_texts, val_target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        val_encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    #encoder_input_data[i, t + 1 :] = input_token_index[\"P\"]\n",
        "    val_encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "\n",
        "    for t, char in enumerate(target_text):\n",
        "      # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        val_decoder_target_data[i, t, target_token_index[char]] = 1.0\n",
        "    val_decoder_target_data[i, t + 1: ,target_token_index[\" \"]] = 1.0"
      ],
      "metadata": {
        "id": "ZBAInKBF2AKY"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "    \n",
        "  def call(self, query, values):\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    \n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "Py8f_D7Z2Cf8"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(LuongAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "   \n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "    \n",
        "    values_transposed = tf.transpose(values, perm=[0, 2, 1])\n",
        "    \n",
        "    score = tf.transpose(tf.matmul(query_with_time_axis, values_transposed) , perm=[0, 2, 1])\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "1ffEjfIH2FVf"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyRNN_atten(object):\n",
        "  def __init__(self,cell_type = 'RNN', hidden_size=32, \n",
        "               learning_rate= 1e-3,dropout=0.3,epochs = 10, batch_size = 32,\n",
        "               attention = 'bahdanau'):\n",
        "    \n",
        "    self.cell_type = cell_type\n",
        "    self.hidden_size = hidden_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.dropout = dropout\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.attention = attention\n",
        "\n",
        "  def build_fit(self,encoder_input_data,decoder_target_data):\n",
        "\n",
        "    encoder_inputs = Input(shape=(max_encoder_seq_length, num_encoder_tokens), name='encoder_inputs')\n",
        "    if self.cell_type == 'LSTM':\n",
        "      encoder_lstm = LSTM(self.hidden_size,return_sequences=True, return_state=True, dropout = self.dropout, name='encoder_lstm')\n",
        "      encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
        "      encoder_states = [encoder_state_h, encoder_state_c]\n",
        "    elif self.cell_type == 'GRU':\n",
        "      encoder_gru = GRU(self.hidden_size,return_sequences=True, return_state=True, dropout = self.dropout, name='encoder_gru')\n",
        "      encoder_outputs, encoder_state_h = encoder_gru(encoder_inputs)\n",
        "      encoder_states = [encoder_state_h]\n",
        "    elif self.cell_type == 'RNN':\n",
        "      encoder_rnn = SimpleRNN(self.hidden_size,return_sequences=True, return_state=True, dropout = self.dropout, name='encoder_rnn')\n",
        "      encoder_outputs, encoder_state_h = encoder_rnn(encoder_inputs)\n",
        "      encoder_states = [encoder_state_h]\n",
        "\n",
        "    # Set up the attention layer\n",
        "    if self.attention == 'bahdanau':\n",
        "      attention= BahdanauAttention(self.hidden_size)\n",
        "    elif self.attention == 'luong':\n",
        "      attention= LuongAttention(self.hidden_size)\n",
        "\n",
        "    # Set up the decoder layers\n",
        "    decoder_inputs = Input(shape=(1, (num_decoder_tokens+self.hidden_size)),name='decoder_inputs')\n",
        "    if self.cell_type == 'LSTM':\n",
        "      decoder_lstm = LSTM(self.hidden_size, dropout = self.dropout, return_state=True, name='decoder_lstm')\n",
        "    elif self.cell_type == 'GRU':\n",
        "      decoder_gru = GRU(self.hidden_size, dropout = self.dropout, return_state=True, name='decoder_gru')\n",
        "    elif self.cell_type == 'RNN':\n",
        "      decoder_rnn = SimpleRNN(self.hidden_size, dropout = self.dropout, return_state=True, name='decoder_rnn')  \n",
        "    \n",
        "    decoder_dense = Dense(num_decoder_tokens, activation='softmax',  name='decoder_dense')\n",
        "\n",
        "    all_outputs = []\n",
        "\n",
        "    inputs = np.zeros((self.batch_size, 1, num_decoder_tokens))\n",
        "    inputs[:, 0, 0] = 1 \n",
        "\n",
        "    decoder_outputs = encoder_state_h\n",
        "    states = encoder_states\n",
        "\n",
        "    for _ in range(max_decoder_seq_length):\n",
        "\n",
        "      context_vector, attention_weights=attention(decoder_outputs, encoder_outputs)\n",
        "      \n",
        "      context_vector = tf.expand_dims(context_vector, 1)\n",
        "      \n",
        "      inputs = tf.concat([context_vector, inputs], axis=-1)\n",
        "      if self.cell_type == 'LSTM':\n",
        "        decoder_outputs, state_h, state_c = decoder_lstm(inputs, initial_state=states)\n",
        "      if self.cell_type == 'GRU':\n",
        "        decoder_outputs, state_h = decoder_gru(inputs, initial_state=states)\n",
        "      if self.cell_type == 'RNN':\n",
        "        decoder_outputs, state_h = decoder_rnn(inputs, initial_state=states)\n",
        "      \n",
        "      outputs = decoder_dense(decoder_outputs)\n",
        "      outputs = tf.expand_dims(outputs, 1)\n",
        "      all_outputs.append(outputs)\n",
        "      inputs = outputs\n",
        "      if self.cell_type == 'LSTM':\n",
        "        states = [state_h, state_c]\n",
        "      if self.cell_type == 'GRU' or self.cell_type == 'RNN':\n",
        "        states = [state_h]\n",
        "\n",
        "\n",
        "    decoder_outputs = Lambda(lambda x: K.concatenate(x, axis=1))(all_outputs)\n",
        "    #getindicelayer = Lambda(lambda x: x[:, -1, :]) \n",
        "    #decoder_outputs = getindicelayer(all_outputs)\n",
        "\n",
        "    model = Model(encoder_inputs, decoder_outputs, name='model_encoder_decoder')\n",
        "    \n",
        "    optimizer = Adam(learning_rate=self.learning_rate, beta_1=0.9, beta_2=0.999)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    #model.summary()\n",
        "\n",
        "    model.fit(encoder_input_data, decoder_target_data,\n",
        "              batch_size=self.batch_size, \n",
        "              epochs=self.epochs,\n",
        "              #callbacks = [WandbCallback()]\n",
        "              )\n",
        "\n",
        "    pred=model.predict(val_encoder_input_data[:6784], batch_size=self.batch_size)\n",
        "\n",
        "    global_count = 0\n",
        "    count = 0\n",
        "    global_total = 0\n",
        "    global_correct = 0\n",
        "    val_total = 6784\n",
        "    for index in range(0, val_total):\n",
        "      one_hot_vector = pred[index]\n",
        "      one_hot_vector1 = val_decoder_target_data[index]\n",
        "      index2 = tf.argmax(one_hot_vector, axis=1)\n",
        "      index1 = tf.argmax(one_hot_vector1, axis=1)\n",
        "      #a = (index2-index1).numpy()\n",
        "      if (index2.numpy() == index1.numpy()).all():\n",
        "        global_correct = global_correct + 1\n",
        "        \n",
        "      global_total = global_total + 1\n",
        "      accuracy_epoch = global_correct/global_total\n",
        "      if global_total % 50 == 0:\n",
        "        wandb.log({'epoch_accuracy' : accuracy_epoch})\n",
        "      #print(\"Accuracy: %s\" % (accuracy_epoch))\n",
        "    \n",
        "    val_accuracy = global_correct/global_total\n",
        "    #print(val_accuracy)\n",
        "\n",
        "    wandb.log({'val_accuracy' : val_accuracy})"
      ],
      "metadata": {
        "id": "69hT1aAj2Hem"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'bayes', \n",
        "    'metric': {\n",
        "      'name': 'val_accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "\n",
        "        'dropout': {\n",
        "            'values': [0.0, 0.1, 0.2]\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-3, 1e-4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64, 128]\n",
        "        },\n",
        "        'hidden_size':{\n",
        "            'values': [32, 64, 128]\n",
        "        },\n",
        "        'cell_type': {\n",
        "            'values': ['RNN', 'GRU', 'LSTM']\n",
        "        },\n",
        "        'attention': {\n",
        "            'values': ['bahdanau', 'luong']\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "89VZVG5o2KyE"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sweep():\n",
        "  config_defaults = {\n",
        "        'dropout': 0.3,\n",
        "        'learning_rate': 1e-3,\n",
        "        'batch_size': 128,\n",
        "        'epochs' : 10,\n",
        "        'hidden_size': 128,\n",
        "        'cell_type': 'LSTM',\n",
        "        'attention': 'bahdanau'\n",
        "        }\n",
        "\n",
        "  # Initialize a new wandb run\n",
        "  wandb.init(config = config_defaults)\n",
        "  \n",
        "  # Config is a variable that holds and saves hyperparameters and inputs\n",
        "  config = wandb.config\n",
        "\n",
        "  wandb.run.name = str(config.cell_type)+ '_' + config.attention +'_bs_'+str(config.batch_size)+'_bs_'+str(config.hidden_size)\n",
        "  \n",
        "  model_rnn = MyRNN_atten(cell_type = config.cell_type, hidden_size=config.hidden_size,\n",
        "                learning_rate= config.learning_rate, dropout=config.dropout,epochs = config.epochs,\n",
        "                batch_size = config.batch_size, attention = config.attention)\n",
        "  \n",
        "  model_rnn.build_fit(encoder_input_data,decoder_target_data)"
      ],
      "metadata": {
        "id": "eGcqLYFe2ceq"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"jyothiraditya\", project=\"assignment3_attn\")\n",
        "wandb.agent(sweep_id, train_sweep, count=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615,
          "referenced_widgets": [
            "66547759188441c3aea3deeded141ceb",
            "9a22ae25cfdb4f98a95748a598831e83",
            "e56c903275a24762ad46f0d4fc5e9e77",
            "47c91e040dfa4d15bdc54c01f52088bf",
            "ca71125cf37442c58fb137f9e88c1e62",
            "d2e46bd2b31a43f0aa8c4f3c70fb8d02",
            "7181ff4308e642bfb4d699d7b1ced2fc",
            "d99fc56c41d44409a9f7ddca38ad8b10"
          ]
        },
        "id": "5tWJ83F52mMw",
        "outputId": "baa0ea77-16bf-47c8-8eca-891f88646382"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: s3rw6dy2\n",
            "Sweep URL: https://wandb.ai/jyothiraditya/assignment3_attn/sweeps/s3rw6dy2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ih27eeac with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: luong\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220507_113033-ih27eeac</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jyothiraditya/assignment3_attn/runs/ih27eeac\" target=\"_blank\">proud-sweep-1</a></strong> to <a href=\"https://wandb.ai/jyothiraditya/assignment3_attn\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/jyothiraditya/assignment3_attn/sweeps/s3rw6dy2\" target=\"_blank\">https://wandb.ai/jyothiraditya/assignment3_attn/sweeps/s3rw6dy2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "457/458 [============================>.] - ETA: 0s - loss: 2.0436 - accuracy: 0.5675"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66547759188441c3aea3deeded141ceb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">proud-sweep-1</strong>: <a href=\"https://wandb.ai/jyothiraditya/assignment3_attn/runs/ih27eeac\" target=\"_blank\">https://wandb.ai/jyothiraditya/assignment3_attn/runs/ih27eeac</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220507_113033-ih27eeac/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ih27eeac errored: InvalidArgumentError()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wh18zl2m with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: luong\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220507_113541-wh18zl2m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/jyothiraditya/assignment3_attn/runs/wh18zl2m\" target=\"_blank\">rural-sweep-2</a></strong> to <a href=\"https://wandb.ai/jyothiraditya/assignment3_attn\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/jyothiraditya/assignment3_attn/sweeps/s3rw6dy2\" target=\"_blank\">https://wandb.ai/jyothiraditya/assignment3_attn/sweeps/s3rw6dy2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 64/458 [===>..........................] - ETA: 3:49 - loss: 2.0566 - accuracy: 0.5766"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8U_Ix6ZS2o8l"
      },
      "execution_count": 128,
      "outputs": []
    }
  ]
}